{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizing Handwritten Digits using Scikit-Learn\n",
    "\n",
    "MNIST dataset: https://en.wikipedia.org/wiki/MNIST_database\n",
    "\n",
    "![alt text][img]\n",
    "\n",
    "[img]: https://i.imgur.com/2xVHT7a.png \"not the best handwriting\"\n",
    "\n",
    "* 28x28 (784 pixels total) images of handwritten digits\n",
    "* each image has a label 0-9\n",
    "* ~~70,000~~ 42,000 total images & labels\n",
    "* our goal is to correctly guess the label when we can only see pixel data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../MNIST/train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f5cfe8ac50ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# mnist = fetch_mldata('MNIST original') # this ridiculously convenient function is broken ;_;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../MNIST/train.csv'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# only using train, because test has no labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\MLenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\MLenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\MLenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\MLenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\MLenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'../MNIST/train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets # broken - mldata.org is down. Using local csv file for now (sorry!)\n",
    "import pandas as pd\n",
    "from IPython.display import display # pretty tables\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# mnist = fetch_mldata('MNIST original') # this ridiculously convenient function is broken ;_;\n",
    "mnist = pd.read_csv('../MNIST/train.csv') # only using train, because test has no labels\n",
    "\n",
    "print(mnist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Rundown\n",
    "\n",
    "\"Machine Learning is the science (and art) of programming computers so they can *learn from data.*\"\n",
    "\n",
    "* Aurélien Géron, *Hands-On Machine Learning with Scikit-Learn & Tensorflow*\n",
    "\n",
    "**Some useful resources**\n",
    "\n",
    "* Scikit-Learn's introduction to machine learning: http://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "* MIT OpenCourseWare: https://www.youtube.com/watch?v=h0e2HAPTGF4\n",
    "* kaggle - datasets and ML competitions: https://www.kaggle.com/datasets\n",
    "\n",
    "**Example: Spam-filter**\n",
    "\n",
    "* Learns to flag spam from given examples of spam emails and regular, \"ham\", emails\n",
    "* This set of example emails is called the *training set*\n",
    "* Each email in the training set is called an *instance*\n",
    "* Each instance contains *data* (contents of the email) and a *target* ('spam' or 'ham')\n",
    "* Each point of data within an instance is a *feature*\n",
    "* For training purposes, we separate the data and target\n",
    "\n",
    "**Math notation**\n",
    "\n",
    "* Uppercase Bold = Matrices\n",
    "    * e.g. $\\mathbf{A}$  \n",
    "* Lowercase Bold = Vectors  \n",
    "    * e.g. $\\mathbf{a}$\n",
    "* Italics = Scalars  \n",
    "    * e.g. $a$\n",
    "* Rows are indexed by subscript, Columns by superscript\n",
    "\n",
    "$\\mathbf{X}$ is a matrix containing the data portion of our dataset (no labels)\n",
    "\n",
    "* $\\mathbf{X}$ has $n$ rows (instances) and $m$ columns (features)\n",
    "\n",
    "The i-th row of the matrix, $\\mathbf{x}_i$, is one instance in our dataset.\n",
    "\n",
    "The j-th element of the i-th row is the value of one feature for a given instance, denoted $x_i^j$\n",
    "\n",
    "$\\mathbf{y}$ is the target vector containing a label for each instance \n",
    "\n",
    "The i-th element of the target vector is a label for a single instance, $y_i$\n",
    "\n",
    "* Sometimes our data doesn't have labels. When we have labels for training, it's called *Supervised Learning*. When we don't, it's *Unsupervised Learning.* We'll stick to Supervised Learning for now.\n",
    "\n",
    "Our model is a function, $h$, which takes in an instance, $\\mathbf{x}_i$, and gives us a *Predicted Label*, $\\hat{y_i}$\n",
    "\n",
    "$$\\large{ h(\\mathbf{x}_i) = \\hat{y_i} }$$\n",
    "\n",
    "if $\\hat{y_i} = y_i$, then our model guessed the label perfectly for the i-th instance\n",
    "\n",
    "We can also run our model on all instanes in our dataset in one go:\n",
    "\n",
    "$$\\large{ h(\\mathbf{X}) = \\mathbf{\\hat{y}} }$$\n",
    "\n",
    "In practice, we split the given dataset into a *Training Set* and *Test Set*, so we can evaluate our models after we train them.\n",
    "*It is very important that we do not let our models see the test data during training!* Our goal is not to memorize our data, but to create a model capable of classifyig data it has never seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate data and target labels\n",
    "\n",
    "* Pandas dataframe/series documentation: https://pandas.pydata.org/pandas-docs/stable/dsintro.html\n",
    "* iloc takes in positional index (ints) and returns a dataframe or series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mnist.iloc[:,1:] # returns a numpy array\n",
    "target = mnist.iloc[:,0]\n",
    "# 784 features (pixels), 42,000 instances (images)\n",
    "print(\"data shape: \", data.shape)\n",
    "print(\"target shape: \", target.shape)\n",
    "print(type(data))\n",
    "print(type(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test/Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.values # converts to numpy array\n",
    "y = target.values\n",
    "print(type(X))\n",
    "print(type(y))\n",
    "# 37,000 for training, 5,000 for testing\n",
    "X_train, X_test = X[:37000], X[37000:]\n",
    "y_train, y_test = y[:37000], y[37000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display A Random Training Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_index = np.random.randint(0, X_train.shape[0])\n",
    "print('Showing image ', rand_index)\n",
    "plt.imshow(X_train[rand_index].reshape(28,28), cmap='Greys')\n",
    "# cmap keyword: defines colorspace of image\n",
    "# https://matplotlib.org/examples/color/colormaps_reference.html\n",
    "plt.axis('off')\n",
    "print('Label: ', target[rand_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classifier\n",
    "\n",
    "Easy starting point. Output is True or False (in the class, or not in the class)\n",
    "\n",
    "In this case, we'll recognize whether a digit is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to restructure our labels (no need to modify the data itself)\n",
    "y_train_5 = (y_train == 5) \n",
    "y_test_5 = (y_test == 5)\n",
    "\n",
    "# let's take a peak at our new training labels\n",
    "print(y_train[0:10])   # multiclass\n",
    "print(y_train_5[0:10]) # binary\n",
    "# now, instead of digits 0-9, we have 0's for 'not 5' and 1's for '5' - Perfect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn\n",
    "\n",
    "* list of classification models: http://scikit-learn.org/stable/supervised_learning.html\n",
    "* To start, we will use a SGDClassifier http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "    - SGD is the name of the training algorithm, \"Stochastic Gradient Descent.\" We'll learn much more about Gradient Descent soon!\n",
    "    - This is a linear model, meaning it finds linear boundaries between classes (remember, our classes to start out with are 'five' and 'not five')\n",
    "    - The main reason I've selected this model is that it's simplicity allows for much faster training on our high-dimensional dataset. The trade-off is that it will be less accurate than more complex models.\n",
    "* To train models in sklearn, we only have to call the fit() function and pass in our data and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# there are many parameters we can set here, but they all have default values, so we can safely leave this blank for now\n",
    "classifier = SGDClassifier() \n",
    "classifier.fit(X_train, y_train_5) # This line alone trains the model. We need to pass in the data and labels\n",
    "# We'll implement our own fit() function soon!\n",
    "\n",
    "# ignore the warning - the default parameters for this class work fine for our purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_index = np.random.randint(0, 5000)\n",
    "# once the model has been trained, we only have to call the predict() function!\n",
    "prediction = classifier.predict([X_test[rand_index]]) # pass in a random row from X_test\n",
    "\n",
    "print(\"Index \", rand_index, \":\")\n",
    "print(\"Predicted Label: \", prediction)\n",
    "print(\"Actual Label (binary): \", y_test_5[rand_index])\n",
    "print(\"Actual Label (multi-class): \", y_test[rand_index])\n",
    "\n",
    "plt.imshow(X_test[rand_index].reshape(28,28), cmap='Greys')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, it worked!! Or maybe it didn't. Either way, one example from the test set doesn't tell us much, and we isolated 5,000 instances for a reason. Fortunately, we can run predict() on the entire test set and get back an array of 5,000 predicted labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_test_5_pred = classifier.predict(X_test) # pass in the entire test set\n",
    "score = accuracy_score(y_test_5, y_test_5_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's a great score! Guess our simple model did pretty well on it's own? Not necessarily.\n",
    "\n",
    "Just because we are doing a binary classification task doesn't mean that our data is split evenly into two classes! In this case, only 10% of the samples are in the 'five' class, and the rest are 'not five', so a \"dumb\" model that just spits out [ False ] Every time would have *90% accuracy!*\n",
    "\n",
    "This is called *Label Imbalance*\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "We can't rely on a single metric to evaluate our model. The *Confusion Matrix* is a useful tool for evaluation, because it shows us how many instances are classified correctly and incorrectly for both of our classes. More precisely,\n",
    "\n",
    "$$\\text{confusion matrix} = \\begin{bmatrix} \\text{True Negative} & \\text{False Positive} \\\\ \\text{False Negative} & \\text{True Positive} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mx = confusion_matrix(y_test_5, y_test_5_pred)\n",
    "print(conf_mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision and Recall\n",
    "\n",
    "Two more useful values we can extract from the confusion matrix are *Precision* and *Recall.*\n",
    "\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "* Precision is the proportion of instances that were correctly labeled as 'True' out of all the 'True' predictions\n",
    "    - How many of our selected items were classified correctly?\n",
    "    - Higher precision => fewer false positives\n",
    "* Recall is the proportion of instances that were correctly labeled as 'True' out of all the instances that actually are 'True'\n",
    "    - How many of the total relevant items did we get?\n",
    "    - Higher recall => fewer false negatives\n",
    "    \n",
    "More formally,\n",
    "\n",
    "$\\text{Precision} = \\large{\\frac{\\text{True Positive}}{\\text{True Positive + False Positive}}}$\n",
    "\n",
    "$\\text{Recall} = \\large{\\frac{\\text{True Positive}}{\\text{True Positive + False Negative}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(y_test_5, y_test_5_pred)\n",
    "recall = recall_score(y_test_5, y_test_5_pred)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_clf = SGDClassifier()\n",
    "multi_clf.fit(X_train, y_train) # same data, different labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy, Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = multi_clf.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "conf_mx = confusion_matrix(y_test, y_test_pred) # what does the confusion matrix look like for multiclass predictions?\n",
    "\n",
    "print(acc) # How is this accuracy score different from the binary accuracy score? \n",
    "           # Is it a better or worse measure of performance?\n",
    "display(pd.DataFrame(conf_mx)) # convert to dataframe so it prints with indices, use display() to make it pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show color map for reference\n",
    "plt.imshow(np.arange(0, 1, 0.01).reshape(1,-1), cmap = 'gnuplot')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(X_train[np.random.randint(0, 5000)].reshape(28,28), cmap='gnuplot')\n",
    "plt.show()\n",
    "\n",
    "# Display confusion matrix as an image\n",
    "plt.imshow(conf_mx, cmap = 'gnuplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision and Recall for Multiclass Predictions\n",
    "\n",
    "//TODO: explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_weighted = precision_score(y_test, y_test_pred, average=\"weighted\")\n",
    "recall_weighted = recall_score(y_test, y_test_pred, average=\"weighted\")\n",
    "precision_macro = precision_score(y_test, y_test_pred, average=\"macro\")\n",
    "recall_macro = recall_score(y_test, y_test_pred, average=\"macro\")\n",
    "\n",
    "print(\"Precision (weighted avg): \", precision_weighted)\n",
    "print(\"Recall (weighted avg): \", recall_weighted) \n",
    "print(\"Precision (simple avg): \", precision_macro)\n",
    "print(\"Recall (simple avg): \", recall_macro) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
