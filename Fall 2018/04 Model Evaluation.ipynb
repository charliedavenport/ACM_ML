{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation & Fine-Tuning\n",
    "* Choosing the right model, and improving its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()\n",
    "print(type(boston)) # A 'Bunch' is a special obj in python, similar to a dict\n",
    "print(boston.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = boston['feature_names']\n",
    "boston_df = pd.DataFrame(boston['data'], columns=features)\n",
    "boston_df['MEDV'] = pd.Series(boston['target'])\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.Series(boston['target'])\n",
    "print('min, max = ', target.min(), ',', target.max())\n",
    "print('mean = ', target.mean())\n",
    "print('std = ', target.std())\n",
    "\n",
    "# target values are in thousands of USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too many features to neatly visualize in a scatter mx. Let's pick a few we think will be good predictors\n",
    "features_we_care_abt = ['RM', 'NOX', 'DIS', 'TAX', 'MEDV']\n",
    "scatter_matrix(boston_df[features_we_care_abt], figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mx = boston_df.corr()\n",
    "corr_mx['MEDV'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = shuffle(boston['data'], boston['target'])\n",
    "\n",
    "X_train, X_test = data[:400], data[400:]\n",
    "y_train, y_test = target[:400], target[400:]\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "d_tree = DecisionTreeRegressor()\n",
    "\n",
    "lin_reg.fit(X_train, y_train)\n",
    "d_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lin = lin_reg.predict(X_test)\n",
    "y_pred_tree = d_tree.predict(X_test)\n",
    "\n",
    "def RMSE(MSE):\n",
    "    \"\"\"Root-Mean-Squared Error\"\"\"\n",
    "    return np.sqrt(abs(MSE))\n",
    "\n",
    "MSE_lin = metrics.mean_squared_error(y_test, y_pred_lin)\n",
    "MSE_tree = metrics.mean_squared_error(y_test, y_pred_tree)\n",
    "\n",
    "print('LinReg MSE: ', MSE_lin)\n",
    "print('D Tree MSE: ', MSE_tree)\n",
    "print()\n",
    "print('LinReg RMSE: ', RMSE(MSE_lin))\n",
    "print('D Tree RMSE: ', RMSE(MSE_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores: \", scores)\n",
    "    print(\"Mean: \", scores.mean())\n",
    "    print(\"Std Dev: \", scores.std())\n",
    "    print(\"Mean RMSE: \", RMSE(scores.mean()))\n",
    "\n",
    "scores_tree = cross_val_score(d_tree, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "scores_lin = cross_val_score(lin_reg, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(\"Decision Tree:\")\n",
    "display_scores(scores_tree)\n",
    "print(\"\\nLinear Regression:\")\n",
    "display_scores(scores_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'max_depth': [2,5,None], 'min_samples_leaf': [1, 5, 10, 20]}\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(d_tree, param_grid, cv=5, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree = grid_search.best_estimator_\n",
    "best_tree.fit(X_train, y_train) # refit on whole training set\n",
    "y_pred_gridSearch = best_tree.predict(X_test)\n",
    "\n",
    "cv_res = pd.DataFrame(grid_search.cv_results_)\n",
    "display(cv_res[['params', 'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']])\n",
    "\n",
    "print('Grid Search MSE: ', metrics.mean_squared_error(y_test, y_pred_gridSearch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "* Averaging predictions from several models to get a better result\n",
    "* \"Wisdom of the crowd\"\n",
    "\n",
    "**Random Forest**: A collection of Decision Trees, all with randomized hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "scores_forest = cross_val_score(forest, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "display_scores(scores_forest)\n",
    "\n",
    "y_pred_forest = forest.predict(X_test)\n",
    "MSE_forest = metrics.mean_squared_error(y_test, y_pred_forest)\n",
    "print('Test MSE: ', MSE_forest)\n",
    "print('Test RMSE: ', RMSE(MSE_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search CV on Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_forest = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2,4,6,8]}\n",
    "]\n",
    "\n",
    "grid_search_forest = GridSearchCV(forest, param_grid_forest, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search_forest.best_params_)\n",
    "print(grid_search_forest.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res_forest = pd.DataFrame(grid_search_forest.cv_results_)\n",
    "display(cv_res_forest[['params', 'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search_forest.best_estimator_.feature_importances_\n",
    "feature_importances = dict(sorted(zip(feature_importances, features), reverse=True))\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try re-training our model on only the 'most important' features\n",
    "data_pruned = boston_df[['LSTAT', 'RM', 'INDUS', 'PTRATIO']]\n",
    "data_pruned = data_pruned.values\n",
    "X_train_pruned, X_test_pruned = data_pruned[:400], data_pruned[400:]\n",
    "print(X_train_pruned.shape, X_test_pruned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_p = RandomForestRegressor(max_features=6, n_estimators=30)\n",
    "forest = RandomForestRegressor(max_features=6, n_estimators=30)\n",
    "\n",
    "scores_forest = cross_val_score(forest, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "scores_forest_p = cross_val_score(forest_p, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "print('Pruned dataset: ')\n",
    "display_scores(scores_forest_p)\n",
    "print()\n",
    "print('Unmodified dataset: ')\n",
    "display_scores(scores_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final performance evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_p.fit(X_train, y_train)\n",
    "y_pred = forest_p.predict(X_test)\n",
    "\n",
    "MSE = metrics.mean_squared_error(y_test, y_pred)\n",
    "print('final MSE: ', MSE)\n",
    "print('final RMSE: ', RMSE(MSE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
